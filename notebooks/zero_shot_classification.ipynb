{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle \n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForTokenClassification, pipeline    \n",
    ")\n",
    "\n",
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIES = [\n",
    "    'social distancing', 'wearing mask', 'breaking rule', 'discrimination', 'vaccination', 'economic impact', 'covid-19 outbreak', \n",
    "    'panic buying', 'government and health authorities', 'being compassionate and helpful', 'school reopening', \n",
    "    'teaching children at home', 'alcohol or drug abuse', 'domestic violence'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_replace(text, page):\n",
    "    new_text = []\n",
    "    text = text.replace('\\n', ' ').replace(\"\\'s\", \"'s\").replace(page, '').strip()\n",
    "    refs = True\n",
    "\n",
    "    for t in text.split(\" \"):\n",
    "        if (t.startswith('@') or t.startswith('http')) and refs:\n",
    "            pass\n",
    "        else:\n",
    "            refs = False\n",
    "            t = '[NAME]' if t.startswith('@') and len(t) > 1 else '' if t.startswith('http') else t\n",
    "            new_text.append(t)\n",
    "        \n",
    "    return \" \".join(new_text).strip()\n",
    "\n",
    "def NER_replace(text):\n",
    "    end_name = -2\n",
    "    replace_names = []\n",
    "    ner_result = xlm_roberta_nlp(text)\n",
    "    for name in ner_result:\n",
    "        if name['entity'] == 'I-PER':\n",
    "            if name['start'] <= end_name+1:\n",
    "                replace_names[-1] = ''.join([replace_names[-1], name['word'].replace('▁', ' ')])\n",
    "                end_name = name['end']\n",
    "            else:\n",
    "                replace_names = replace_names + [name['word'].replace('▁', ' ')]\n",
    "                end_name = name['end']\n",
    "    replace_names = [name.strip() for name in replace_names]\n",
    "\n",
    "    for name in replace_names:\n",
    "        text = text.replace(name, '[NAME]')\n",
    "\n",
    "def render_text(comment, page, NER=False):\n",
    "    \n",
    "    text = comment.content\n",
    "    text = token_replace(text, page)\n",
    "    if NER:\n",
    "        text = NER_replace(text, page)        \n",
    "    return text\n",
    "\n",
    "def get_elegible(comment):\n",
    "    \n",
    "    text = comment.renderedContent    \n",
    "    comment_criteria = len(text.split()) > 3\n",
    "    words_criteria = (len(text) / (len(text.split())+1)) > 2\n",
    "    lang_criteria = comment.lang == 'en'\n",
    "    \n",
    "    return comment_criteria and words_criteria and lang_criteria\n",
    "\n",
    "def get_zero_shot_classification(tweet):\n",
    "        \n",
    "    if tweet.elegible == False:\n",
    "        return ''\n",
    "    \n",
    "    text = tweet.renderedContent\n",
    "    result = facebook_zsc(text, CATEGORIES, multi_label=True)\n",
    "    tmp = pd.DataFrame(columns=result['labels'])\n",
    "    tmp.loc[0] = result['scores']\n",
    "    return [tweet.tweetId] + list(tmp[CATEGORIES].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Facebook Zero Shot Classifier\n",
    "facebook_zsc = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start processing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-c8a151c4a8da>:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tmp['renderedContent'] = tmp.apply(lambda x: render_text(x, '@'+page), axis=1)\n",
      "<ipython-input-5-c8a151c4a8da>:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tmp['elegible'] = tmp.apply(lambda x: get_elegible(x), axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: ABC | Time: 5592.094683647156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-c8a151c4a8da>:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tmp['renderedContent'] = tmp.apply(lambda x: render_text(x, '@'+page), axis=1)\n",
      "<ipython-input-5-c8a151c4a8da>:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tmp['elegible'] = tmp.apply(lambda x: get_elegible(x), axis=1)\n"
     ]
    }
   ],
   "source": [
    "news_tweets = pd.read_parquet('./../data/raw/news_tweets.parquet')\n",
    "news_accounts = pd.read_parquet('./../data/raw/news_accounts.parquet')\n",
    "news_tweets_zsc = pd.DataFrame([])\n",
    "print('Start processing.')\n",
    "for user in news_tweets.userId.unique():\n",
    "    start = time.time()\n",
    "    page = news_accounts[news_accounts.userId==user].username.values[0]\n",
    "    tmp = news_tweets[news_tweets.userId==user]\n",
    "    tmp['renderedContent'] = tmp.apply(lambda x: render_text(x, '@'+page), axis=1)\n",
    "    tmp['elegible'] = tmp.apply(lambda x: get_elegible(x), axis=1)\n",
    "    zsc = tmp.apply(lambda x: get_zero_shot_classification(x), axis=1)\n",
    "    zsc = pd.DataFrame(list(zsc), columns=['tweetId'] + CATEGORIES)\n",
    "    zsc = zsc[~(zsc.tweetId.isnull())]\n",
    "    zsc.to_parquet(page + '_zsc.parquet')\n",
    "    news_tweets_zsc = news_tweets_zsc.append(zsc)\n",
    "    print('Page:', page, '| Time:', time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
